### NIPS 2018: Interesting papers with one line summary.

## CNN Architechture
	1. [Pelee: A Real-Time Object Detection System on Mobile Devices](http://papers.nips.cc/paper/7466-pelee-a-real-time-object-detection-system-on-mobile-devices.pdf)
		- An efficient architecture named PeleeNet, which is built with conventional convolution. PeeleeNet is higher in accuracy and 1.8 times faster that MobileNet and MobileNetV2 on ImageNet dataset with only 66% model size.
	2. [Moonshine: Distilling with Cheap Convolutions](http://papers.nips.cc/paper/7553-moonshine-distilling-with-cheap-convolutions.pdf) 
		- We propose structural model distillation for memory reduction using a strategy that produces a student architecture that is a simple transformation of the teacher architecture: no redesign is needed, and the same hyperparameters can be used.
	3. [ChannelNets: Compact and Efficient Convolutional Neural Networks via Channel-Wise Convolutions](http://papers.nips.cc/paper/7766-channelnets-compact-and-efficient-convolutional-neural-networks-via-channel-wise-convolutions.pdf) 
		- We propose to compress deep models (ChannelNets) by using channel-wise convolutions, which replace dense connections among feature maps with sparse ones in CNNs. Notably, our work represents the first attempt to compress the fully-connected classification layer.
	4. [Heterogeneous Bitwidth Binarization in Convolutional Neural Networks](http://papers.nips.cc/paper/7656-heterogeneous-bitwidth-binarization-in-convolutional-neural-networks.pdf)
		- We show that it is feasible and useful to select bitwidths (with right mix of 1-,2- and 3-bit) at the parameter granularity during training instead of the whole model have the same low bidwidth (e.g. 2 bits)

## Communication- or Memory bounded learning
	1. [GradiVeQ: Vector Quantization for Bandwidth-Efficient Gradient Aggregation in Distributed CNN Training](https://papers.nips.cc/paper/7759-gradiveq-vector-quantization-for-bandwidth-efficient-gradient-aggregation-in-distributed-cnn-training.pdf)
		- We empirically demonstrate the strong linear correlations between CNN gradients, and propose a gradient vector quantization technique, named GradiVeQ, to exploit these correlations through principal component analysis (PCA) for substantial gradient dimension reduction. Reduced the end-to-end distributed training time by almost 50%.
	2. [Gradient Sparsification for Communication-Efficient Distributed Optimization](http://papers.nips.cc/paper/7405-gradient-sparsification-for-communication-efficient-distributed-optimization.pdf)
		- To reduce the comminication cost, We propose a convex optimization formulation to minimize the coding length of stochastic gradients. 
	3. [ATOMO: Communication-efficient Learning via Atomic Sparsification](http://papers.nips.cc/paper/8191-atomo-communication-efficient-learning-via-atomic-sparsification.pdf)
		- We present ATOMO, a general framework for atomic sparsification of stochastic gradients. Given a gradient, an atomic decomposition, and a sparsity budget, ATOMO gives a random unbiased sparsification of the atoms minimizing variance.
	4. [Pipe-SGD: A Decentralized Pipelined SGD Framework for Distributed Deep Net Training](http://papers.nips.cc/paper/8028-pipe-sgd-a-decentralized-pipelined-sgd-framework-for-distributed-deep-net-training.pdf)
		- Recently, distributed training with AllReduce operations gained popularity. we carefully analyze the AllReduce based setup, propose timing models which include network latency, bandwidth, cluster size and compute time, and demonstrate that a pipelined training with a width of two combines the best of both synchronous and asynchronous training.

## Efficient Methods
	1. [Scalable methods for 8-bit training of neural networks](http://papers.nips.cc/paper/7761-scalable-methods-for-8-bit-training-of-neural-networks.pdf)
		- We quantize the model parameters, activations and layer gradients to 8-bit, leaving at higher precision only the final step in the computation of the weight gradients.
	2. [A Simple Cache Model for Image Recognition](http://papers.nips.cc/paper/8214-a-simple-cache-model-for-image-recognition.pdf)	
		- The key observation we make is that the layers of a deep network close to the output layer contain independent, easily extractable class-relevant information that is not contained in the output layer itself. We propose to extract this extra class-relevant information using a simple key-value cache memory to improve the classification performance of the model at test time.
	3. [Sparsified SGD with Memory](http://papers.nips.cc/paper/7697-sparsified-sgd-with-memory.pdf)
		- We analyze Stochastic Gradient Descent (SGD) with k-sparsification or compression (for instance top-k or random-k) and show that this scheme converges at the same rate as vanilla SGD when equipped with error compensation (keeping track of accumulated errors in memory) 
	4. [A Linear Speedup Analysis of Distributed Deep Learning with Sparse and Quantized Communication](http://papers.nips.cc/paper/7519-a-linear-speedup-analysis-of-distributed-deep-learning-with-sparse-and-quantized-communication.pdf)
		- We study the convergence rate of distributed SGD for non-convex optimization with two communication reducing strategies: sparse parameter averaging and gradient quantization. 
	5. [BML: A High-performance, Low-cost Gradient Synchronization Algorithm for DML Training](http://papers.nips.cc/paper/7678-bml-a-high-performance-low-cost-gradient-synchronization-algorithm-for-dml-training.pdf)
		- We propose BML, a new gradient synchronization algorithm with higher network performance and lower network cost than the current practice. BML runs on BCube network, instead of using the traditional Fat-Tree topology
	6. [Training Deep Neural Networks with 8-bit Floating Point Numbers](http://papers.nips.cc/paper/7994-training-deep-neural-networks-with-8-bit-floating-point-numbers.pdf)	
		- We demonstrate, for the first time, the successful training of deep neural networks using 8-bit floating point numbers while fully maintaining the accuracy on a spectrum of deep learning models and datasets. 
	7. [Communication Compression for Decentralized Training](http://papers.nips.cc/paper/7992-communication-compression-for-decentralized-training.pdf)
		- We explore a natural question: can the combination of communication compression and decentralization techniques lead to a system that is robust to both bandwidth and latency?
	8. [Distributed Stochastic Optimization via Adaptive SGD](https://papers.nips.cc/paper/7461-distributed-stochastic-optimization-via-adaptive-sgd.pdf)
		- We propose an efficient distributed stochastic optimization method by combining adaptivity with variance reduction techniques.
	9. [Efficient Stochastic Gradient Hard Thresholding](https://papers.nips.cc/paper/7469-efficient-stochastic-gradient-hard-thresholding.pdf)
		- We propose an efficient hybrid stochastic gradient hard thresholding (HSG-HT) method that can be provably shown to have sample-size-independent gradient evaluation and hard thresholding complexity bounds. 

## Hardware and Software systems
	1. [Learning to Optimize Tensor Programs](https://papers.nips.cc/paper/7599-learning-to-optimize-tensor-programs.pdf)
		- We introduce a learning-based framework to optimize tensor programs for deep learning workloads. 
	2. [Snap ML: A Hierarchical Framework for Machine Learning](http://papers.nips.cc/paper/7309-snap-ml-a-hierarchical-framework-for-machine-learning.pdf)
		- We describe a new software framework for fast training of generalized linear models. The framework, named Snap Machine Learning (Snap ML), combines recent advances in machine learning systems and algorithms in a nested manner to reflect the hierarchical architecture of modern computing systems. 
	3. [Mesh-TensorFlow: Deep Learning for Supercomputers](http://papers.nips.cc/paper/8242-mesh-tensorflow-deep-learning-for-supercomputers.pdf)
		- Model parallelism framework 
	4. [Modular Networks: Learning to Decompose Neural Computation](http://papers.nips.cc/paper/7508-modular-networks-learning-to-decompose-neural-computation.pdf)
		- We propose a training algorithm that flexibly chooses neural modules based on the data to be processed. Both the decomposition and modules are learned end-to-end. 
	5. [Backpropagation with Callbacks: Foundations for Efficient and Expressive Differentiable Programming](http://papers.nips.cc/paper/8221-backpropagation-with-callbacks-foundations-for-efficient-and-expressive-differentiable-programming.pdf)
		- We propose an implementation of backpropagation using functions with callbacks, where the forward pass is executed as a sequence of function calls, and the backward pass as a corresponding sequence of function returns.

## Computational complexity
	1. [The streaming rollout of deep networks - towards fully model-parallel execution](https://papers.nips.cc/paper/7659-the-streaming-rollout-of-deep-networks-towards-fully-model-parallel-execution.pdf)
		- For the training of and inference with recurrent neural networks, they are usually rolled out over time, and different rollouts exist. We present a theoretical framework to describe rollouts, the level of model-parallelization they induce, and demonstrate differences in solving specific tasks. The streaming rollout enables a fully parallel execution of the network reducing runtime on massively parallel devices.
	2. [The Effect of Network Width on the Performance of Large-batch Training](https://papers.nips.cc/paper/8142-the-effect-of-network-width-on-the-performance-of-large-batch-training.pdf)
		- Distributed implementations of mini-batch stochastic gradient descent (SGD) suffer from communication overheads, attributed to the high frequency of gradient updates inherent in small-batch training. We take a first step towards analyzing how the structure (width and depth) of a neural network affects the performance of large-batch training .

## Computer Vision and Video
	1.  [Video-to-Video Synthesis](http://papers.nips.cc/paper/7391-.pdfvideo-to-video-synthesis.pdf)
		- Goal is to learn a mapping function from an input source video to an output photorealistic video.
	2. [VideoCapsuleNet: A Simplified Network for Action Detection](http://papers.nips.cc/paper/7988-videocapsulenet-a-simplified-network-for-action-detection.pdf)
		- We propose a 3D capsule network for videos, called VideoCapsuleNet: a unified network for action detection which can jointly perform pixel-wise action segmentation along with action classification. 

## Misc
	1. [Neural Arithmetic Logic Units](http://papers.nips.cc/paper/8027-neural-arithmetic-logic-units.pdf)
		- A neural arithmetic logic unit (NALU), by analogy to the arithmetic logic unit in traditional processors. An architecture that represents numerical quantities as linear activations which are manipulated using primitive arithmetic operators, controlled by learned gates.
	2. [Distributed Weight Consolidation: A Brain Segmentation Case Study](http://papers.nips.cc/paper/7664-distributed-weight-consolidation-a-brain-segmentation-case-study.pdf)
		- We introduce distributed weight consolidation (DWC), a continual learning method to consolidate the weights of separate neural networks, each trained on an independent dataset.
